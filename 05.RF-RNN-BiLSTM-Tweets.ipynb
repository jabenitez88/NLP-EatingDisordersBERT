{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweets about eating disorders\n",
    "## 05. Training Random Forest, RNN and Bi-LSTM\n",
    "\n",
    "The training and evaluation of Random Forest, RNN and Bi-LSTM models on the 4 existing categorizations in the dataset is shown below:\n",
    "\n",
    "- Category 1: in this category, tweets that have been written by people suffering from ED are represented with a value of 1 and the rest with a value of 0. In order to be able to assess this, each user profile was accessed by looking at the user's description and different tweets published by the user. In this way it was possible to determine which tweets were written by people who publicly mentioned having an ED and which were not.\n",
    "- Category 2: tweets that promoted having an ED were labelled with a value of 1 and all other tweets were labelled with a value of 0. There are communities of people who suffer from EDs who try to encourage other people to also suffer from it by promoting it as if it were something positive or fashionable. There are many studies that talk about pro ED communities and that are detected with terms such as \"proana\" or \"pro-anorexia\" 9.\n",
    "- Category 3: in this category, informative tweets were represented with a value of 1 and non-informative tweets with a value of 0. Informative tweets are those that show information with the aim of informing readers, while the rest are written texts in which the author reflects a subjective opinion.\n",
    "- Category 4: in category 4, scientific tweets were labelled with a value of 1 and the rest with a value of 0. A tweet of an informative nature that had been written by a person belonging to the field of research, for example, a doctor of philosophy in different subjects, was labelled as a scientific tweet. Scientific tweets were also those that shared links to articles published in scientific journals.\n",
    "\n",
    "For the selection of Random Forest hyperparameters, a search of the best hyperparameters for each of the categories is performed using GridSearchCV applying a 5-fold cross-validation.\n",
    "\n",
    "The generated RNN and Bi-LSTM models were selected after performing several tests with other configurations that reflected lower performance for the problem presented in this research.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MICROSOFT\\Anaconda3\\envs\\ED\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3457: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version.\n",
      "\n",
      "\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "from nltk import word_tokenize, sent_tokenize, FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
    "nltk.download\n",
    "tweets = pd.read_csv('tweets_cleaned.csv', encoding='utf8', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>stream_group</th>\n",
       "      <th>text_orig</th>\n",
       "      <th>Commercial</th>\n",
       "      <th>POLITICS</th>\n",
       "      <th>ED</th>\n",
       "      <th>Family</th>\n",
       "      <th>ED_patience</th>\n",
       "      <th>ProED</th>\n",
       "      <th>Offensive</th>\n",
       "      <th>Informative</th>\n",
       "      <th>Scientific</th>\n",
       "      <th>Sad</th>\n",
       "      <th>hashtag</th>\n",
       "      <th>text</th>\n",
       "      <th>Segmented#</th>\n",
       "      <th>text_similar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>RT @beatED: Learn more about anorexia and buli...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['BBCPanorama']</td>\n",
       "      <td>['learn', 'anorexia', 'bulimia', 'well', 'eati...</td>\n",
       "      <td>bbc panorama</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>A woman tries to balance her relationships wit...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>['anorexia', 'BodyofWater']</td>\n",
       "      <td>['woman', 'try', 'balance', 'relationship', 'm...</td>\n",
       "      <td>anorexia  bodyof water</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  stream_group  \\\n",
       "0           0             1   \n",
       "1           1             1   \n",
       "\n",
       "                                           text_orig  Commercial  POLITICS  \\\n",
       "0  RT @beatED: Learn more about anorexia and buli...           0         0   \n",
       "1  A woman tries to balance her relationships wit...           0         0   \n",
       "\n",
       "   ED  Family  ED_patience  ProED  Offensive  Informative  Scientific  Sad  \\\n",
       "0   1     0.0            0      0          0            1           0  0.0   \n",
       "1   1     0.0            0      0          0            0           0  0.0   \n",
       "\n",
       "                       hashtag  \\\n",
       "0              ['BBCPanorama']   \n",
       "1  ['anorexia', 'BodyofWater']   \n",
       "\n",
       "                                                text              Segmented#  \\\n",
       "0  ['learn', 'anorexia', 'bulimia', 'well', 'eati...            bbc panorama   \n",
       "1  ['woman', 'try', 'balance', 'relationship', 'm...  anorexia  bodyof water   \n",
       "\n",
       "  text_similar  \n",
       "0           []  \n",
       "1           []  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets.columns=['num1','stream_group','text_orig','f1_commercial','f2_politics','f3_ed','f4_family','f5_edpatient','f6_proed','f7_offensive','f8_info','f9_scientific','f10_sad','hashtag','text','segmented','text_similar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols=['f1_commercial','f2_politics','f3_ed','f4_family','f5_edpatient','f6_proed','f7_offensive','f8_info','f9_scientific','f10_sad']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1 = tweets.copy().drop(['f1_commercial', 'f2_politics', 'f3_ed', 'f4_family', 'f7_offensive', 'f10_sad'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\MICROSOFT\\Anaconda3\\envs\\ED\\lib\\site-packages\\torchvision\\io\\image.py:11: UserWarning: Failed to load image Python extension: [WinError 126] No se puede encontrar el módulo especificado\n",
      "  warn(f\"Failed to load image Python extension: {e}\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import spacy\n",
    "import nltk\n",
    "import nltk.data\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "import regex as re\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "from simpletransformers.classification import ClassificationModel\n",
    "\n",
    "\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuations = \"¡!#$%&'()*+,-./:;<=>¿?@[\\]^_`{|}~\"\n",
    "\n",
    "def read_txt(filename):\n",
    "    list = []\n",
    "    with open(filename, 'r', encoding='utf-8') as f:\n",
    "        data = f.readlines()\n",
    "        for line in data:\n",
    "            list.append(str(line).replace('\\n', ''))\n",
    "    return list\n",
    "\n",
    "stopwords = read_txt('english_stopwords.txt')\n",
    "\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "def clean_accents(tweet):\n",
    "    tweet = re.sub(r\"[àáâãäå]\", \"a\", tweet)\n",
    "    tweet = re.sub(r\"ç\", \"c\", tweet)\n",
    "    tweet = re.sub(r\"[èéêë]\", \"e\", tweet)\n",
    "    tweet = re.sub(r\"[ìíîï]\", \"i\", tweet)\n",
    "    tweet = re.sub(r\"[òóôõö]\", \"o\", tweet)\n",
    "    tweet = re.sub(r\"[ùúûü]\", \"u\", tweet)\n",
    "    tweet = re.sub(r\"[ýÿ]\", \"y\", tweet)\n",
    "\n",
    "    return tweet\n",
    "\n",
    "def clean_tweet(tweet, stem = False):\n",
    "    tweet = tweet.lower().strip()\n",
    "    tweet = re.sub(r'https?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'http?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'www?:\\/\\/\\S+', '', tweet)\n",
    "    tweet = re.sub(r'\\s([@#][\\w_-]+)', \"\", tweet)\n",
    "    tweet = re.sub(r\"\\n\", \" \", tweet)\n",
    "    tweet = clean_accents(tweet)\n",
    "    tweet = re.sub(r\"\\b(a*ha+h[ha]*|o?l+o+l+[ol]*|x+d+[x*d*]*|a*ja+[j+a+]+)\\b\", \"<risas>\", tweet)\n",
    "    for symbol in punctuations:\n",
    "        tweet = tweet.replace(symbol, \"\")\n",
    "    tokens = []\n",
    "    for token in tweet.strip().split():\n",
    "        if token not in punctuations and token not in stopwords:\n",
    "            if stem:\n",
    "                tokens.append(stemmer.stem(token))\n",
    "            else:\n",
    "                tokens.append(token)\n",
    "    return \" \".join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets1['text_cleaned'] = tweets['text_orig'].apply(lambda s : clean_tweet(s))\n",
    "#print(tweets1['text_cleaned'].head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tweets1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweets1.copy()\n",
    "X = df['text_cleaned']\n",
    "Y1 = df['f5_edpatient']\n",
    "Y2 = df['f6_proed']\n",
    "Y3 = df['f8_info']\n",
    "Y4 = df['f9_scientific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X, Y4, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    0.51773\n",
       "0    0.48227\n",
       "Name: f5_edpatient, dtype: float64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y1_test.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\MICROSOFT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\MICROSOFT\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "documents = []\n",
    "\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidfconverter = TfidfTransformer()\n",
    "X = tfidfconverter.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X, Y4, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 7, 'max_features': 'log2', 'n_estimators': 200}\n",
      "{'criterion': 'gini', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 1000}\n",
      "{'criterion': 'gini', 'max_depth': 8, 'max_features': 'sqrt', 'n_estimators': 800}\n",
      "{'criterion': 'gini', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_jobs=-1,max_features= 'sqrt' ,n_estimators=50, oob_score = True) \n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [200, 700, 800, 1000, 1200],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'max_depth' : [4,5,6,7,8],\n",
    "    'criterion' :['gini', 'entropy']\n",
    "}\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X, Y1)\n",
    "print(CV_rfc.best_params_)\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X, Y2)\n",
    "print(CV_rfc.best_params_)\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X, Y3)\n",
    "print(CV_rfc.best_params_)\n",
    "\n",
    "CV_rfc = GridSearchCV(estimator=rfc, param_grid=param_grid, cv= 5)\n",
    "CV_rfc.fit(X, Y4)\n",
    "print(CV_rfc.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[230  42]\n",
      " [ 57 235]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.85      0.82       272\n",
      "           1       0.85      0.80      0.83       292\n",
      "\n",
      "    accuracy                           0.82       564\n",
      "   macro avg       0.82      0.83      0.82       564\n",
      "weighted avg       0.83      0.82      0.82       564\n",
      "\n",
      "0.824468085106383\n",
      "accuracy:  0.7916482269503545\n",
      "f1:  0.7976027744860016\n",
      "--- 1.8179750442504883 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "import time\n",
    "start_time = time.time()\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "classifier = RandomForestClassifier(criterion='gini',max_depth=7,max_features='log2',n_estimators=200, random_state=42)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "rfc_cv_score = cross_validate(classifier, X, Y1, cv=5, scoring=['accuracy','f1'])\n",
    "print('accuracy: ',rfc_cv_score['test_accuracy'].mean())\n",
    "print('f1: ',rfc_cv_score['test_f1'].mean())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[427   0]\n",
      " [135   2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      1.00      0.86       427\n",
      "           1       1.00      0.01      0.03       137\n",
      "\n",
      "    accuracy                           0.76       564\n",
      "   macro avg       0.88      0.51      0.45       564\n",
      "weighted avg       0.82      0.76      0.66       564\n",
      "\n",
      "0.7606382978723404\n",
      "accuracy:  0.7671801418439717\n",
      "f1:  0.0474154747867857\n",
      "--- 13.491974830627441 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# {'criterion': 'gini', 'max_depth': 8, 'max_features': 'auto', 'n_estimators': 1000}\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "start_time = time.time()\n",
    "classifier = RandomForestClassifier(criterion='gini',max_depth=8,max_features='auto',n_estimators=1000, random_state=42)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "rfc_cv_score = cross_validate(classifier, X, Y2, cv=5, scoring=['accuracy','f1'])\n",
    "print('accuracy: ',rfc_cv_score['test_accuracy'].mean())\n",
    "print('f1: ',rfc_cv_score['test_f1'].mean())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[375   3]\n",
      " [102  84]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.99      0.88       378\n",
      "           1       0.97      0.45      0.62       186\n",
      "\n",
      "    accuracy                           0.81       564\n",
      "   macro avg       0.88      0.72      0.75       564\n",
      "weighted avg       0.85      0.81      0.79       564\n",
      "\n",
      "0.8138297872340425\n",
      "accuracy:  0.7373531914893617\n",
      "f1:  0.49162946182890765\n",
      "--- 10.476030826568604 seconds ---\n"
     ]
    }
   ],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "start_time = time.time()\n",
    "classifier = RandomForestClassifier(criterion='gini',max_depth=8,max_features='sqrt',n_estimators=800, random_state=42)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "rfc_cv_score = cross_validate(classifier, X, Y3, cv=5, scoring=['accuracy','f1'])\n",
    "print('accuracy: ',rfc_cv_score['test_accuracy'].mean())\n",
    "print('f1: ',rfc_cv_score['test_f1'].mean())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[442   1]\n",
      " [107  14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      1.00      0.89       443\n",
      "           1       0.93      0.12      0.21       121\n",
      "\n",
      "    accuracy                           0.81       564\n",
      "   macro avg       0.87      0.56      0.55       564\n",
      "weighted avg       0.83      0.81      0.74       564\n",
      "\n",
      "0.8085106382978723\n",
      "accuracy:  0.8039460992907801\n",
      "f1:  0.2731931152912991\n",
      "--- 13.271032810211182 seconds ---\n"
     ]
    }
   ],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y4, test_size=0.3, random_state=42)\n",
    "start_time = time.time()\n",
    "classifier = RandomForestClassifier(criterion='gini',max_depth=8,max_features='auto',n_estimators=1000, random_state=42)\n",
    "classifier.fit(X1_train, y1_train) \n",
    "\n",
    "y1_pred = classifier.predict(X1_test)\n",
    "\n",
    "print(confusion_matrix(y1_test,y1_pred))\n",
    "print(classification_report(y1_test,y1_pred))\n",
    "print(accuracy_score(y1_test, y1_pred))\n",
    "rfc_cv_score = cross_validate(classifier, X, Y4, cv=5, scoring=['f1','accuracy'])\n",
    "print('accuracy: ',rfc_cv_score['test_accuracy'].mean())\n",
    "print('f1: ',rfc_cv_score['test_f1'].mean())\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model\n",
    "from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = tweets1.copy()\n",
    "X = df['text_cleaned']\n",
    "Y1 = df['f5_edpatient']\n",
    "Y2 = df['f6_proed']\n",
    "Y3 = df['f8_info']\n",
    "Y4 = df['f9_scientific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X, Y4, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RNN():\n",
    "    inputs = Input(name='inputs',shape=[max_len])\n",
    "    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n",
    "    layer = LSTM(100)(layer)\n",
    "    layer = Dense(256,name='FC1')(layer)\n",
    "    layer = Activation('relu')(layer)\n",
    "    layer = Dropout(0.1)(layer)\n",
    "    layer = Dense(1,name='out_layer')(layer)\n",
    "    layer = Activation('sigmoid')(layer)\n",
    "    model = Model(inputs=inputs,outputs=layer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 150)]             0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, 150, 50)           50000     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100)               60400     \n",
      "                                                                 \n",
      " FC1 (Dense)                 (None, 256)               25856     \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " out_layer (Dense)           (None, 1)                 257       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 136,513\n",
      "Trainable params: 136,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "model = RNN()\n",
    "model.summary()\n",
    "#plot_model(model, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 150)]             0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 150, 50)           50000     \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 100)               60400     \n",
      "                                                                 \n",
      " FC1 (Dense)                 (None, 256)               25856     \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " out_layer (Dense)           (None, 1)                 257       \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 136,513\n",
      "Trainable params: 136,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0.0:\n",
    "        return 0.0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score\n",
    "\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy',f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "17/17 [==============================] - 4s 107ms/step - loss: 0.6775 - accuracy: 0.6295 - f1_score: nan - val_loss: 0.5807 - val_accuracy: 0.8251 - val_f1_score: 0.8255\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 0.5050 - accuracy: 0.7971 - f1_score: 0.7805 - val_loss: 0.3674 - val_accuracy: 0.8479 - val_f1_score: 0.8411\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1f687736888>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(sequences_matrix,y1_train,batch_size=64,epochs=20,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18/18 [==============================] - 0s 14ms/step - loss: 0.4452 - accuracy: 0.8014 - f1_score: 0.7981\n"
     ]
    }
   ],
   "source": [
    "accr = model.evaluate(test_sequences_matrix,y1_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "  Loss: 0.445\n",
      "  Accuracy: 0.801\n",
      " f1: 0.798\n"
     ]
    }
   ],
   "source": [
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n f1: {:0.3f}'.format(accr[0],accr[1],accr[2]))\n",
    "dft = pd.DataFrame({'Text':X1_test,'Actual':y1_test,'Predicted':y1_pred})\n",
    "dft.to_csv('predictions/cate1-bert-total-preds.csv', encoding='utf-8')\n",
    "dft[dft['Actual'] != dft['Predicted']].to_csv('predictions/cate1-rnn-total-wrong_preds.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 150)]             0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, 150, 50)           50000     \n",
      "                                                                 \n",
      " lstm_3 (LSTM)               (None, 100)               60400     \n",
      "                                                                 \n",
      " FC1 (Dense)                 (None, 256)               25856     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " out_layer (Dense)           (None, 1)                 257       \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 136,513\n",
      "Trainable params: 136,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "17/17 [==============================] - 3s 93ms/step - loss: 0.5917 - accuracy: 0.7448 - f1_score: nan - val_loss: 0.5419 - val_accuracy: 0.7871 - val_f1_score: nan\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 1s 71ms/step - loss: 0.4607 - accuracy: 0.7648 - f1_score: nan - val_loss: 0.5317 - val_accuracy: 0.7072 - val_f1_score: 0.5911\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.5499 - accuracy: 0.6649 - f1_score: 0.5693\n",
      "Test set\n",
      "  Loss: 0.550\n",
      "  Accuracy: 0.665\n",
      " f1: 0.569\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 2\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy',f1_score])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=64,epochs=20,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n f1: {:0.3f}'.format(accr[0],accr[1],accr[2]))\n",
    "\n",
    "dft = pd.DataFrame({'Text':X1_test,'Actual':y1_test,'Predicted':y1_pred})\n",
    "dft.to_csv('predictions/cate2-rnn-total-preds.csv', encoding='utf-8')\n",
    "dft[dft['Actual'] != dft['Predicted']].to_csv('predictions/cate2-rnn-total-wrong_preds.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 150)]             0         \n",
      "                                                                 \n",
      " embedding_4 (Embedding)     (None, 150, 50)           50000     \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 100)               60400     \n",
      "                                                                 \n",
      " FC1 (Dense)                 (None, 256)               25856     \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " out_layer (Dense)           (None, 1)                 257       \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 136,513\n",
      "Trainable params: 136,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "17/17 [==============================] - 3s 91ms/step - loss: 0.6666 - accuracy: 0.6152 - f1_score: nan - val_loss: 0.6257 - val_accuracy: 0.6046 - val_f1_score: nan\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 1s 68ms/step - loss: 0.5476 - accuracy: 0.7390 - f1_score: nan - val_loss: 0.4366 - val_accuracy: 0.8555 - val_f1_score: 0.7817\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.4641 - accuracy: 0.8103 - f1_score: 0.7009\n",
      "Test set\n",
      "  Loss: 0.464\n",
      "  Accuracy: 0.810\n",
      " f1: 0.701\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 3\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy',f1_score])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=64,epochs=20,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n f1: {:0.3f}'.format(accr[0],accr[1],accr[2]))\n",
    "matrix = sklearn.metrics.confusion_matrix(y1_test, y1_pred)\n",
    "matrix\n",
    "\n",
    "dft = pd.DataFrame({'Text':X1_test,'Actual':y1_test,'Predicted':y1_pred})\n",
    "dft[dft['Actual'] != dft['Predicted']]\n",
    "dft.to_csv('predictions/cate3-rnn-total-preds.csv', encoding='utf-8')\n",
    "dft[dft['Actual'] != dft['Predicted']].to_csv('predictions/cate3-rnn-total-wrong_preds.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " inputs (InputLayer)         [(None, 150)]             0         \n",
      "                                                                 \n",
      " embedding_5 (Embedding)     (None, 150, 50)           50000     \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 100)               60400     \n",
      "                                                                 \n",
      " FC1 (Dense)                 (None, 256)               25856     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " out_layer (Dense)           (None, 1)                 257       \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 136,513\n",
      "Trainable params: 136,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "17/17 [==============================] - 4s 101ms/step - loss: 0.5996 - accuracy: 0.7371 - f1_score: nan - val_loss: 0.5416 - val_accuracy: 0.7376 - val_f1_score: nan\n",
      "Epoch 2/20\n",
      "17/17 [==============================] - 1s 74ms/step - loss: 0.4565 - accuracy: 0.7724 - f1_score: nan - val_loss: 0.5796 - val_accuracy: 0.7376 - val_f1_score: nan\n",
      "18/18 [==============================] - 0s 15ms/step - loss: 0.5202 - accuracy: 0.7872 - f1_score: nan\n",
      "Test set\n",
      "  Loss: 0.520\n",
      "  Accuracy: 0.787\n",
      " f1: nan\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 4\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y4, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "max_words = 1000\n",
    "max_len = 150\n",
    "tok = Tokenizer(num_words=max_words)\n",
    "tok.fit_on_texts(X1_train)\n",
    "sequences = tok.texts_to_sequences(X1_train)\n",
    "sequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)\n",
    "model = RNN()\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy',f1_score])\n",
    "model.fit(sequences_matrix,y1_train,batch_size=64,epochs=20,\n",
    "          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])\n",
    "test_sequences = tok.texts_to_sequences(X1_test)\n",
    "test_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)\n",
    "accr = model.evaluate(test_sequences_matrix,y1_test)\n",
    "print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}\\n f1: {:0.3f}'.format(accr[0],accr[1],accr[2]))\n",
    "matrix = sklearn.metrics.confusion_matrix(y1_test, y1_pred)\n",
    "matrix\n",
    "\n",
    "dft = pd.DataFrame({'Text':X1_test,'Actual':y1_test,'Predicted':y1_pred})\n",
    "dft[dft['Actual'] != dft['Predicted']]\n",
    "\n",
    "dft.to_csv('predictions/cate4-rnn-total-preds.csv', encoding='utf-8')\n",
    "dft[dft['Actual'] != dft['Predicted']].to_csv('predictions/cate4-rnn-total-wrong_preds.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rt learn more about anorexia and bulimia as well as other eating disorders here'"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "42/42 [==============================] - 7s 53ms/step - loss: 0.6893 - accuracy: 0.5050 - f1_score: nan - val_loss: 0.6855 - val_accuracy: 0.4823 - val_f1_score: nan\n",
      "Epoch 2/14\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.6691 - accuracy: 0.5050 - f1_score: nan - val_loss: 0.6486 - val_accuracy: 0.4823 - val_f1_score: nan\n",
      "Epoch 3/14\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.5627 - accuracy: 0.5545 - f1_score: nan - val_loss: 0.5044 - val_accuracy: 0.7234 - val_f1_score: 0.6436\n",
      "Epoch 4/14\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.3998 - accuracy: 0.8294 - f1_score: 0.8142 - val_loss: 0.4986 - val_accuracy: 0.7500 - val_f1_score: 0.6905\n",
      "Epoch 5/14\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.3138 - accuracy: 0.8728 - f1_score: 0.8622 - val_loss: 0.5115 - val_accuracy: 0.7801 - val_f1_score: 0.7343\n",
      "Epoch 6/14\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.2752 - accuracy: 0.8835 - f1_score: 0.8510 - val_loss: 0.4604 - val_accuracy: 0.8227 - val_f1_score: 0.8167\n",
      "Epoch 7/14\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.2371 - accuracy: 0.9078 - f1_score: 0.9016 - val_loss: 0.5282 - val_accuracy: 0.8014 - val_f1_score: 0.7735\n",
      "Epoch 8/14\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.2071 - accuracy: 0.9269 - f1_score: 0.9245 - val_loss: 0.5634 - val_accuracy: 0.8032 - val_f1_score: 0.7799\n",
      "Epoch 9/14\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.1760 - accuracy: 0.9315 - f1_score: 0.9283 - val_loss: 0.5752 - val_accuracy: 0.8067 - val_f1_score: 0.7965\n",
      "Epoch 10/14\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.1636 - accuracy: 0.9391 - f1_score: 0.9147 - val_loss: 0.7140 - val_accuracy: 0.7784 - val_f1_score: 0.7423\n",
      "Epoch 11/14\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1445 - accuracy: 0.9490 - f1_score: 0.9249 - val_loss: 0.6680 - val_accuracy: 0.8014 - val_f1_score: 0.7818\n",
      "Epoch 12/14\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1148 - accuracy: 0.9604 - f1_score: 0.9601 - val_loss: 0.7429 - val_accuracy: 0.7943 - val_f1_score: 0.7736\n",
      "Epoch 13/14\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.0999 - accuracy: 0.9642 - f1_score: 0.9624 - val_loss: 0.7966 - val_accuracy: 0.7890 - val_f1_score: 0.7698\n",
      "Epoch 14/14\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.0850 - accuracy: 0.9703 - f1_score: 0.9708 - val_loss: 0.8800 - val_accuracy: 0.7872 - val_f1_score: 0.7725\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.8800 - accuracy: 0.7872 - f1_score: 0.7807\n",
      "Test Loss: 0.8800280690193176\n",
      "Test Accuracy: 0.7872340679168701\n",
      "Test f1: 0.7806693911552429\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 1\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y1, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['accuracy',f1_score])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "test_loss, test_acc, test_f1 = model.evaluate(X1_test,y1_test)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "print('Test f1: {}'.format(test_f1))\n",
    "\n",
    "\n",
    "matrix = sklearn.metrics.confusion_matrix(y1_test, y1_pred)\n",
    "matrix\n",
    "\n",
    "dft = pd.DataFrame({'Text':X1_test,'Actual':y1_test,'Predicted':y1_pred})\n",
    "dft[dft['Actual'] != dft['Predicted']]\n",
    "\n",
    "dft.to_csv('predictions/cate1-bilstm-total-preds.csv', encoding='utf-8')\n",
    "dft[dft['Actual'] != dft['Predicted']].to_csv('predictions/cate1-bilstm-total-wrong_preds.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "42/42 [==============================] - 7s 54ms/step - loss: 0.6617 - accuracy: 0.7647 - f1_score: nan - val_loss: 0.6005 - val_accuracy: 0.7571 - val_f1_score: nan\n",
      "Epoch 2/14\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.5192 - accuracy: 0.7647 - f1_score: nan - val_loss: 0.4704 - val_accuracy: 0.7571 - val_f1_score: nan\n",
      "Epoch 3/14\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.4534 - accuracy: 0.7647 - f1_score: nan - val_loss: 0.4511 - val_accuracy: 0.7571 - val_f1_score: nan\n",
      "Epoch 4/14\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.4096 - accuracy: 0.7647 - f1_score: nan - val_loss: 0.4057 - val_accuracy: 0.7571 - val_f1_score: nan\n",
      "Epoch 5/14\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.3425 - accuracy: 0.7647 - f1_score: nan - val_loss: 0.3605 - val_accuracy: 0.7571 - val_f1_score: nan\n",
      "Epoch 6/14\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.2654 - accuracy: 0.8035 - f1_score: nan - val_loss: 0.3188 - val_accuracy: 0.8014 - val_f1_score: nan\n",
      "Epoch 7/14\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.2110 - accuracy: 0.8896 - f1_score: 0.7003 - val_loss: 0.3535 - val_accuracy: 0.8369 - val_f1_score: nan\n",
      "Epoch 8/14\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1823 - accuracy: 0.9109 - f1_score: 0.7557 - val_loss: 0.3576 - val_accuracy: 0.8493 - val_f1_score: nan\n",
      "Epoch 9/14\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1531 - accuracy: 0.9307 - f1_score: 0.8113 - val_loss: 0.3716 - val_accuracy: 0.8599 - val_f1_score: nan\n",
      "Epoch 10/14\n",
      "42/42 [==============================] - 0s 11ms/step - loss: 0.1324 - accuracy: 0.9459 - f1_score: 0.8421 - val_loss: 0.4197 - val_accuracy: 0.8617 - val_f1_score: nan\n",
      "Epoch 11/14\n",
      "42/42 [==============================] - 0s 12ms/step - loss: 0.1091 - accuracy: 0.9566 - f1_score: 0.8937 - val_loss: 0.4723 - val_accuracy: 0.8670 - val_f1_score: nan\n",
      "Epoch 12/14\n",
      "42/42 [==============================] - 1s 13ms/step - loss: 0.0953 - accuracy: 0.9657 - f1_score: 0.8975 - val_loss: 0.4958 - val_accuracy: 0.8652 - val_f1_score: nan\n",
      "Epoch 13/14\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.0840 - accuracy: 0.9665 - f1_score: 0.8998 - val_loss: 0.5337 - val_accuracy: 0.8688 - val_f1_score: nan\n",
      "Epoch 14/14\n",
      "42/42 [==============================] - 1s 12ms/step - loss: 0.0776 - accuracy: 0.9711 - f1_score: 0.9139 - val_loss: 0.5379 - val_accuracy: 0.8599 - val_f1_score: nan\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.5379 - accuracy: 0.8599 - f1_score: 0.6714\n",
      "Test Loss: 0.5378828048706055\n",
      "Test Accuracy: 0.859929084777832\n",
      "Test f1: 0.6714297533035278\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 2\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y2, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['accuracy',f1_score])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "test_loss, test_acc, test_f1 = model.evaluate(X1_test,y1_test)\n",
    "\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "print('Test f1: {}'.format(test_f1))\n",
    "\n",
    "matrix = sklearn.metrics.confusion_matrix(y1_test, y1_pred)\n",
    "matrix\n",
    "\n",
    "dft = pd.DataFrame({'Text':X1_test,'Actual':y1_test,'Predicted':y1_pred})\n",
    "dft[dft['Actual'] != dft['Predicted']]\n",
    "\n",
    "dft.to_csv('predictions/cate2-bilstm-total-preds.csv', encoding='utf-8')\n",
    "dft[dft['Actual'] != dft['Predicted']].to_csv('predictions/cate2-bilstm-total-wrong_preds.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "42/42 [==============================] - 7s 55ms/step - loss: 0.6873 - accuracy: 0.6131 - f1_score: nan - val_loss: 0.6715 - val_accuracy: 0.6702 - val_f1_score: nan\n",
      "Epoch 2/14\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.6679 - accuracy: 0.6131 - f1_score: nan - val_loss: 0.6348 - val_accuracy: 0.6702 - val_f1_score: nan\n",
      "Epoch 3/14\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.5982 - accuracy: 0.6161 - f1_score: nan - val_loss: 0.5194 - val_accuracy: 0.7199 - val_f1_score: nan\n",
      "Epoch 4/14\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.4545 - accuracy: 0.8050 - f1_score: nan - val_loss: 0.4812 - val_accuracy: 0.7996 - val_f1_score: nan\n",
      "Epoch 5/14\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.3733 - accuracy: 0.8324 - f1_score: 0.7188 - val_loss: 0.4381 - val_accuracy: 0.8316 - val_f1_score: 0.6967\n",
      "Epoch 6/14\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.3308 - accuracy: 0.8682 - f1_score: 0.7995 - val_loss: 0.4249 - val_accuracy: 0.8333 - val_f1_score: 0.6890\n",
      "Epoch 7/14\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.2687 - accuracy: 0.8995 - f1_score: 0.8330 - val_loss: 0.4449 - val_accuracy: 0.8351 - val_f1_score: 0.7102\n",
      "Epoch 8/14\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.2265 - accuracy: 0.9109 - f1_score: 0.8483 - val_loss: 0.4611 - val_accuracy: 0.8404 - val_f1_score: 0.7132\n",
      "Epoch 9/14\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.1946 - accuracy: 0.9216 - f1_score: 0.8881 - val_loss: 0.5621 - val_accuracy: 0.8138 - val_f1_score: 0.7118\n",
      "Epoch 10/14\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.1589 - accuracy: 0.9406 - f1_score: 0.9190 - val_loss: 0.5978 - val_accuracy: 0.8121 - val_f1_score: 0.6859\n",
      "Epoch 11/14\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.1295 - accuracy: 0.9528 - f1_score: 0.9119 - val_loss: 0.6961 - val_accuracy: 0.8032 - val_f1_score: 0.6869\n",
      "Epoch 12/14\n",
      "42/42 [==============================] - 1s 15ms/step - loss: 0.1107 - accuracy: 0.9612 - f1_score: 0.9214 - val_loss: 0.8068 - val_accuracy: 0.7890 - val_f1_score: 0.6802\n",
      "Epoch 13/14\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.0945 - accuracy: 0.9695 - f1_score: 0.9349 - val_loss: 0.8532 - val_accuracy: 0.7961 - val_f1_score: 0.6410\n",
      "Epoch 14/14\n",
      "42/42 [==============================] - 1s 14ms/step - loss: 0.0842 - accuracy: 0.9711 - f1_score: 0.9604 - val_loss: 0.9885 - val_accuracy: 0.7908 - val_f1_score: 0.6795\n",
      "18/18 [==============================] - 0s 4ms/step - loss: 0.9885 - accuracy: 0.7908 - f1_score: 0.6851\n",
      "Test Loss: 0.9884968400001526\n",
      "Test Accuracy: 0.7907801270484924\n",
      "Test f1: 0.6851006150245667\n"
     ]
    }
   ],
   "source": [
    "# CATEGORY 3\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y3, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['accuracy',f1_score])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "test_loss, test_acc, test_f1 = model.evaluate(X1_test,y1_test)\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "print('Test f1: {}'.format(test_f1))\n",
    "\n",
    "matrix = sklearn.metrics.confusion_matrix(y1_test, y1_pred)\n",
    "matrix\n",
    "\n",
    "dft = pd.DataFrame({'Text':X1_test,'Actual':y1_test,'Predicted':y1_pred})\n",
    "dft[dft['Actual'] != dft['Predicted']]\n",
    "\n",
    "dft.to_csv('predictions/cate3-bilstm-total-preds.csv', encoding='utf-8')\n",
    "dft[dft['Actual'] != dft['Predicted']].to_csv('predictions/cate3-bilstm-total-wrong_preds.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "42/42 [==============================] - 7s 62ms/step - loss: 0.6603 - accuracy: 0.7593 - f1_score: nan - val_loss: 0.5886 - val_accuracy: 0.7855 - val_f1_score: nan\n",
      "Epoch 2/14\n",
      "42/42 [==============================] - 1s 24ms/step - loss: 0.5677 - accuracy: 0.7593 - f1_score: nan - val_loss: 0.5158 - val_accuracy: 0.7855 - val_f1_score: nan\n",
      "Epoch 3/14\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 0.5127 - accuracy: 0.7593 - f1_score: nan - val_loss: 0.4575 - val_accuracy: 0.7855 - val_f1_score: nan\n",
      "Epoch 4/14\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 0.3974 - accuracy: 0.7593 - f1_score: nan - val_loss: 0.3416 - val_accuracy: 0.7855 - val_f1_score: nan\n",
      "Epoch 5/14\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 0.2620 - accuracy: 0.8530 - f1_score: nan - val_loss: 0.3146 - val_accuracy: 0.8883 - val_f1_score: 0.6741\n",
      "Epoch 6/14\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 0.1810 - accuracy: 0.9353 - f1_score: 0.8307 - val_loss: 0.3179 - val_accuracy: 0.8972 - val_f1_score: nan\n",
      "Epoch 7/14\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 0.1420 - accuracy: 0.9482 - f1_score: 0.8544 - val_loss: 0.3241 - val_accuracy: 0.9043 - val_f1_score: 0.7223\n",
      "Epoch 8/14\n",
      "42/42 [==============================] - 1s 24ms/step - loss: 0.1103 - accuracy: 0.9627 - f1_score: 0.9132 - val_loss: 0.3676 - val_accuracy: 0.9025 - val_f1_score: 0.7237\n",
      "Epoch 9/14\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 0.1216 - accuracy: 0.9604 - f1_score: 0.8935 - val_loss: 0.4479 - val_accuracy: 0.8918 - val_f1_score: 0.7014\n",
      "Epoch 10/14\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 0.0806 - accuracy: 0.9733 - f1_score: 0.9230 - val_loss: 0.4227 - val_accuracy: 0.9043 - val_f1_score: 0.7155\n",
      "Epoch 11/14\n",
      "42/42 [==============================] - 1s 26ms/step - loss: 0.0646 - accuracy: 0.9840 - f1_score: 0.9380 - val_loss: 0.4681 - val_accuracy: 0.9060 - val_f1_score: 0.7266\n",
      "Epoch 12/14\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 0.0548 - accuracy: 0.9848 - f1_score: 0.9446 - val_loss: 0.5134 - val_accuracy: 0.9007 - val_f1_score: 0.7205\n",
      "Epoch 13/14\n",
      "42/42 [==============================] - 1s 25ms/step - loss: 0.0462 - accuracy: 0.9878 - f1_score: nan - val_loss: 0.5471 - val_accuracy: 0.8954 - val_f1_score: 0.7057\n",
      "Epoch 14/14\n",
      "16/42 [==========>...................] - ETA: 0s - loss: 0.0328 - accuracy: 0.9922 - f1_score: 0.9780"
     ]
    }
   ],
   "source": [
    "# CATEGORY 4\n",
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X, Y4, test_size=0.3, random_state=42)\n",
    "\n",
    "VOCAB_SIZE=1000\n",
    "encoder = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
    "    max_tokens=VOCAB_SIZE)\n",
    "encoder.adapt(np.asarray(X1_train))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    encoder,\n",
    "    tf.keras.layers.Embedding(\n",
    "        input_dim=len(encoder.get_vocabulary()),\n",
    "        output_dim=64,\n",
    "        # Use masking to handle the variable sequence lengths\n",
    "        mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-4),\n",
    "              metrics=['accuracy',f1_score])\n",
    "\n",
    "history = model.fit(X1_train,y1_train, epochs=14,\n",
    "                    validation_data=(X1_test,y1_test), \n",
    "                    validation_steps=30)\n",
    "\n",
    "test_loss, test_acc, test_f1 = model.evaluate(X1_test,y1_test)\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_acc))\n",
    "print('Test f1: {}'.format(test_f1))\n",
    "\n",
    "matrix = sklearn.metrics.confusion_matrix(y1_test, y1_pred)\n",
    "matrix\n",
    "dft = pd.DataFrame({'Text':X1_test,'Actual':y1_test,'Predicted':y1_pred})\n",
    "\n",
    "dft.to_csv('predictions/cate4-bilstm-total-preds.csv', encoding='utf-8')\n",
    "dft[dft['Actual'] != dft['Predicted']].to_csv('predictions/cate4-bilstm-total-wrong_preds.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1111</th>\n",
       "      <td>developed anorexia years ago even knew edtwt risas</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1448</th>\n",
       "      <td>height bulimia vs recovery therapy learning total food freedom</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1326</th>\n",
       "      <td>tw eating disorders i’m recovery anymore honestly don’t know feel worse relapsing ever thinking could possibly recover pray nobody know irl notices</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>tw ed anorexia uhm irl said \"i went ed worst\" \"bsf\" literally responded \"thank fuck dont issues\" fucking insensitive</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>865</th>\n",
       "      <td>yall bully dont go eat icecream😀😀💔</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>european eating disorders review covid19 issue open access</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>need strict pro ana coach dm pls</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1733</th>\n",
       "      <td>researchbased interventions eating disorders paraphilias</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>isnt funny love starve second family screams dinner binge</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>551</th>\n",
       "      <td>jeez hello ig i’m back ed twt pff rtlike mootslt3 19 theythem cw 116 ugw 88 dni you’re gonna soft offended anything pro ana want well</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>386 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                     Text  \\\n",
       "1111                                                                                                   developed anorexia years ago even knew edtwt risas   \n",
       "1448                                                                                       height bulimia vs recovery therapy learning total food freedom   \n",
       "1326  tw eating disorders i’m recovery anymore honestly don’t know feel worse relapsing ever thinking could possibly recover pray nobody know irl notices   \n",
       "270                                  tw ed anorexia uhm irl said \"i went ed worst\" \"bsf\" literally responded \"thank fuck dont issues\" fucking insensitive   \n",
       "865                                                                                                                    yall bully dont go eat icecream😀😀💔   \n",
       "...                                                                                                                                                   ...   \n",
       "1334                                                                                           european eating disorders review covid19 issue open access   \n",
       "893                                                                                                                      need strict pro ana coach dm pls   \n",
       "1733                                                                                             researchbased interventions eating disorders paraphilias   \n",
       "236                                                                                             isnt funny love starve second family screams dinner binge   \n",
       "551                 jeez hello ig i’m back ed twt pff rtlike mootslt3 19 theythem cw 116 ugw 88 dni you’re gonna soft offended anything pro ana want well   \n",
       "\n",
       "      Actual  Predicted  \n",
       "1111       0          1  \n",
       "1448       0          1  \n",
       "1326       0          1  \n",
       "270        0          1  \n",
       "865        0          1  \n",
       "...      ...        ...  \n",
       "1334       1          0  \n",
       "893        0          1  \n",
       "1733       1          0  \n",
       "236        0          1  \n",
       "551        0          1  \n",
       "\n",
       "[386 rows x 3 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dft = pd.DataFrame({'Text':X1_test,'Actual':y1_test,'Predicted':y1_pred})\n",
    "dft[dft['Actual'] != dft['Predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
